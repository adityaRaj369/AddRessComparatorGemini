{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1pGm_Dv5FE8"
      },
      "source": [
        "# Zero-DCE for low-light image enhancement\n",
        "\n",
        "**Author:** [Soumik Rakshit](http://github.com/soumik12345)<br>\n",
        "**Date created:** 2021/09/18<br>\n",
        "**Last modified:** 2023/07/15<br>\n",
        "**Description:** Implementing Zero-Reference Deep Curve Estimation for low-light image enhancement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3BCX33U5FFC"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "**Zero-Reference Deep Curve Estimation** or **Zero-DCE** formulates low-light image\n",
        "enhancement as the task of estimating an image-specific\n",
        "[*tonal curve*](https://en.wikipedia.org/wiki/Curve_(tonality)) with a deep neural network.\n",
        "In this example, we train a lightweight deep network, **DCE-Net**, to estimate\n",
        "pixel-wise and high-order tonal curves for dynamic range adjustment of a given image.\n",
        "\n",
        "Zero-DCE takes a low-light image as input and produces high-order tonal curves as its output.\n",
        "These curves are then used for pixel-wise adjustment on the dynamic range of the input to\n",
        "obtain an enhanced image. The curve estimation process is done in such a way that it maintains\n",
        "the range of the enhanced image and preserves the contrast of neighboring pixels. This\n",
        "curve estimation is inspired by curves adjustment used in photo editing software such as\n",
        "Adobe Photoshop where users can adjust points throughout an image’s tonal range.\n",
        "\n",
        "Zero-DCE is appealing because of its relaxed assumptions with regard to reference images:\n",
        "it does not require any input/output image pairs during training.\n",
        "This is achieved through a set of carefully formulated non-reference loss functions,\n",
        "which implicitly measure the enhancement quality and guide the training of the network.\n",
        "\n",
        "### References\n",
        "\n",
        "- [Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement](https://arxiv.org/abs/2001.06826)\n",
        "- [Curves adjustment in Adobe Photoshop](https://helpx.adobe.com/photoshop/using/curves-adjustment.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah9PkTk_5FFD"
      },
      "source": [
        "## Downloading LOLDataset\n",
        "\n",
        "The **LoL Dataset** has been created for low-light image enhancement. It provides 485\n",
        "images for training and 15 for testing. Each image pair in the dataset consists of a\n",
        "low-light input image and its corresponding well-exposed reference image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zmabDLLT5FFE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KDTeqji45FFG",
        "outputId": "ca4b7b2c-53a7-4ef5-dce4-bf0ef90a40a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-28 07:20:13--  https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.55, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/d9/09/d909ef7668bb417b7065a311bd55a3084cc83a1f918e13cb41c5503328432db2/419fddc48958cd0f5599939ee0248852a37ceb8bb738c9b9525e95b25a89de9a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lol_dataset.zip%3B+filename%3D%22lol_dataset.zip%22%3B&response-content-type=application%2Fzip&Expires=1753690813&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzY5MDgxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kOS8wOS9kOTA5ZWY3NjY4YmI0MTdiNzA2NWEzMTFiZDU1YTMwODRjYzgzYTFmOTE4ZTEzY2I0MWM1NTAzMzI4NDMyZGIyLzQxOWZkZGM0ODk1OGNkMGY1NTk5OTM5ZWUwMjQ4ODUyYTM3Y2ViOGJiNzM4YzliOTUyNWU5NWIyNWE4OWRlOWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=W%7ExKYY7p673855kmG9vGy4L9PKWllQrJjfwCzi--K7-oYyrkf2UXv%7ErEhYtzGC4ymQ9NxzJfiGdYO%7E7NWpjn9XyDBVrWYj9vyBRjHiXwYFXekAuuzjqeoy7qRQ-r-fLX8rCHr2PgYf86Hcv1HGLwC-UbJsf20yP8Z8sVtyjXp%7E8IN9jipHZd6Ed-2ECV5Bzc7nRzPq6TUQ-gSpwywEoR0LIh3YsM%7E386llJGHgBxsTcp-SzKKzRtRMEHxajXX4CLjd3x4O-79GnUWJipV-fNweUdvEf8uuDa5x1X05FOhCsmCVzqMXCvGR7qcn8JYQUzBnQTdzliKYWKGevSng%7E6Ww__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-07-28 07:20:13--  https://cdn-lfs.hf.co/repos/d9/09/d909ef7668bb417b7065a311bd55a3084cc83a1f918e13cb41c5503328432db2/419fddc48958cd0f5599939ee0248852a37ceb8bb738c9b9525e95b25a89de9a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27lol_dataset.zip%3B+filename%3D%22lol_dataset.zip%22%3B&response-content-type=application%2Fzip&Expires=1753690813&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MzY5MDgxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9kOS8wOS9kOTA5ZWY3NjY4YmI0MTdiNzA2NWEzMTFiZDU1YTMwODRjYzgzYTFmOTE4ZTEzY2I0MWM1NTAzMzI4NDMyZGIyLzQxOWZkZGM0ODk1OGNkMGY1NTk5OTM5ZWUwMjQ4ODUyYTM3Y2ViOGJiNzM4YzliOTUyNWU5NWIyNWE4OWRlOWE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=W%7ExKYY7p673855kmG9vGy4L9PKWllQrJjfwCzi--K7-oYyrkf2UXv%7ErEhYtzGC4ymQ9NxzJfiGdYO%7E7NWpjn9XyDBVrWYj9vyBRjHiXwYFXekAuuzjqeoy7qRQ-r-fLX8rCHr2PgYf86Hcv1HGLwC-UbJsf20yP8Z8sVtyjXp%7E8IN9jipHZd6Ed-2ECV5Bzc7nRzPq6TUQ-gSpwywEoR0LIh3YsM%7E386llJGHgBxsTcp-SzKKzRtRMEHxajXX4CLjd3x4O-79GnUWJipV-fNweUdvEf8uuDa5x1X05FOhCsmCVzqMXCvGR7qcn8JYQUzBnQTdzliKYWKGevSng%7E6Ww__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.169.231.38, 3.169.231.4, 3.169.231.115, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.169.231.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 347171015 (331M) [application/zip]\n",
            "Saving to: ‘lol_dataset.zip’\n",
            "\n",
            "lol_dataset.zip     100%[===================>] 331.09M   194MB/s    in 1.7s    \n",
            "\n",
            "2025-07-28 07:20:15 (194 MB/s) - ‘lol_dataset.zip’ saved [347171015/347171015]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/geekyrakshit/LoL-Dataset/resolve/main/lol_dataset.zip\n",
        "!unzip -q lol_dataset.zip && rm lol_dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADa1XoDq5FFG"
      },
      "source": [
        "## Creating a TensorFlow Dataset\n",
        "\n",
        "We use 300 low-light images from the LoL Dataset training set for training, and we use\n",
        "the remaining 185 low-light images for validation. We resize the images to size `256 x\n",
        "256` to be used for both training and validation. Note that in order to train the DCE-Net,\n",
        "we will not require the corresponding enhanced images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "z57rplCQ5FFG",
        "outputId": "cdb52f11-ea28-410d-fcd8-c6e996d0fc26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n",
            "Validation Dataset: <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "MAX_TRAIN_IMAGES = 400\n",
        "\n",
        "\n",
        "def load_data(image_path):\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_png(image, channels=3)\n",
        "    image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def data_generator(low_light_images):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "train_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "val_low_light_images = sorted(glob(\"./lol_dataset/our485/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "test_low_light_images = sorted(glob(\"./lol_dataset/eval15/low/*\"))\n",
        "\n",
        "\n",
        "train_dataset = data_generator(train_low_light_images)\n",
        "val_dataset = data_generator(val_low_light_images)\n",
        "\n",
        "print(\"Train Dataset:\", train_dataset)\n",
        "print(\"Validation Dataset:\", val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uTrxTsp5FFH"
      },
      "source": [
        "## The Zero-DCE Framework\n",
        "\n",
        "The goal of DCE-Net is to estimate a set of best-fitting light-enhancement curves\n",
        "(LE-curves) given an input image. The framework then maps all pixels of the input’s RGB\n",
        "channels by applying the curves iteratively to obtain the final enhanced image.\n",
        "\n",
        "### Understanding light-enhancement curves\n",
        "\n",
        "A ligh-enhancement curve is a kind of curve that can map a low-light image\n",
        "to its enhanced version automatically,\n",
        "where the self-adaptive curve parameters are solely dependent on the input image.\n",
        "When designing such a curve, three objectives should be taken into account:\n",
        "\n",
        "- Each pixel value of the enhanced image should be in the normalized range `[0,1]`, in order to\n",
        "avoid information loss induced by overflow truncation.\n",
        "- It should be monotonous, to preserve the contrast between neighboring pixels.\n",
        "- The shape of this curve should be as simple as possible,\n",
        "and the curve should be differentiable to allow backpropagation.\n",
        "\n",
        "The light-enhancement curve is separately applied to three RGB channels instead of solely on the\n",
        "illumination channel. The three-channel adjustment can better preserve the inherent color and reduce\n",
        "the risk of over-saturation.\n",
        "\n",
        "![](https://li-chongyi.github.io/Zero-DCE_files/framework.png)\n",
        "\n",
        "### DCE-Net\n",
        "\n",
        "The DCE-Net is a lightweight deep neural network that learns the mapping between an input\n",
        "image and its best-fitting curve parameter maps. The input to the DCE-Net is a low-light\n",
        "image while the outputs are a set of pixel-wise curve parameter maps for corresponding\n",
        "higher-order curves. It is a plain CNN of seven convolutional layers with symmetrical\n",
        "concatenation. Each layer consists of 32 convolutional kernels of size 3×3 and stride 1\n",
        "followed by the ReLU activation function. The last convolutional layer is followed by the\n",
        "Tanh activation function, which produces 24 parameter maps for 8 iterations, where each\n",
        "iteration requires three curve parameter maps for the three channels.\n",
        "\n",
        "![](https://i.imgur.com/HtIg34W.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tOP10zAy5FFI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_dce_net():\n",
        "    input_img = keras.Input(shape=[None, None, 3])\n",
        "    conv1 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(input_img)\n",
        "    conv2 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv1)\n",
        "    conv3 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv2)\n",
        "    conv4 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(conv3)\n",
        "    int_con1 = layers.Concatenate(axis=-1)([conv4, conv3])\n",
        "    conv5 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(int_con1)\n",
        "    int_con2 = layers.Concatenate(axis=-1)([conv5, conv2])\n",
        "    conv6 = layers.Conv2D(\n",
        "        32, (3, 3), strides=(1, 1), activation=\"relu\", padding=\"same\"\n",
        "    )(int_con2)\n",
        "    int_con3 = layers.Concatenate(axis=-1)([conv6, conv1])\n",
        "    x_r = layers.Conv2D(24, (3, 3), strides=(1, 1), activation=\"tanh\", padding=\"same\")(\n",
        "        int_con3\n",
        "    )\n",
        "    return keras.Model(inputs=input_img, outputs=x_r)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSE5BqSL5FFI"
      },
      "source": [
        "## Loss functions\n",
        "\n",
        "To enable zero-reference learning in DCE-Net, we use a set of differentiable\n",
        "zero-reference losses that allow us to evaluate the quality of enhanced images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_NuHAJT5FFJ"
      },
      "source": [
        "### Color constancy loss\n",
        "\n",
        "The *color constancy loss* is used to correct the potential color deviations in the\n",
        "enhanced image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "THKytykL5FFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def color_constancy_loss(x):\n",
        "    mean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\n",
        "    mr, mg, mb = (\n",
        "        mean_rgb[:, :, :, 0],\n",
        "        mean_rgb[:, :, :, 1],\n",
        "        mean_rgb[:, :, :, 2],\n",
        "    )\n",
        "    d_rg = tf.square(mr - mg)\n",
        "    d_rb = tf.square(mr - mb)\n",
        "    d_gb = tf.square(mb - mg)\n",
        "    return tf.sqrt(tf.square(d_rg) + tf.square(d_rb) + tf.square(d_gb))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoSNT4us5FFJ"
      },
      "source": [
        "### Exposure loss\n",
        "\n",
        "To restrain under-/over-exposed regions, we use the *exposure control loss*.\n",
        "It measures the distance between the average intensity value of a local region\n",
        "and a preset well-exposedness level (set to `0.6`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5oiJPSeM5FFJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def exposure_loss(x, mean_val=0.6):\n",
        "    x = tf.reduce_mean(x, axis=3, keepdims=True)\n",
        "    mean = tf.nn.avg_pool2d(x, ksize=16, strides=16, padding=\"VALID\")\n",
        "    return tf.reduce_mean(tf.square(mean - mean_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLx9T-TG5FFJ"
      },
      "source": [
        "### Illumination smoothness loss\n",
        "\n",
        "To preserve the monotonicity relations between neighboring pixels, the\n",
        "*illumination smoothness loss* is added to each curve parameter map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lHAwWXOo5FFK"
      },
      "outputs": [],
      "source": [
        "\n",
        "def illumination_smoothness_loss(x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    h_x = tf.shape(x)[1]\n",
        "    w_x = tf.shape(x)[2]\n",
        "    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
        "    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
        "    h_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\n",
        "    w_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\n",
        "    batch_size = tf.cast(batch_size, dtype=tf.float32)\n",
        "    count_h = tf.cast(count_h, dtype=tf.float32)\n",
        "    count_w = tf.cast(count_w, dtype=tf.float32)\n",
        "    return 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNGgQ5dX5FFK"
      },
      "source": [
        "### Spatial consistency loss\n",
        "\n",
        "The *spatial consistency loss* encourages spatial coherence of the enhanced image by\n",
        "preserving the contrast between neighboring regions across the input image and its enhanced version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SFnvmFQD5FFK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class SpatialConsistencyLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(reduction=\"none\")\n",
        "\n",
        "        self.left_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.right_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.up_kernel = tf.constant(\n",
        "            [[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "        self.down_kernel = tf.constant(\n",
        "            [[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n",
        "        )\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        original_mean = tf.reduce_mean(y_true, 3, keepdims=True)\n",
        "        enhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\n",
        "        original_pool = tf.nn.avg_pool2d(\n",
        "            original_mean, ksize=4, strides=4, padding=\"VALID\"\n",
        "        )\n",
        "        enhanced_pool = tf.nn.avg_pool2d(\n",
        "            enhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n",
        "        )\n",
        "\n",
        "        d_original_left = tf.nn.conv2d(\n",
        "            original_pool,\n",
        "            self.left_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        d_original_right = tf.nn.conv2d(\n",
        "            original_pool,\n",
        "            self.right_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        d_original_up = tf.nn.conv2d(\n",
        "            original_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_original_down = tf.nn.conv2d(\n",
        "            original_pool,\n",
        "            self.down_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "\n",
        "        d_enhanced_left = tf.nn.conv2d(\n",
        "            enhanced_pool,\n",
        "            self.left_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        d_enhanced_right = tf.nn.conv2d(\n",
        "            enhanced_pool,\n",
        "            self.right_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "        d_enhanced_up = tf.nn.conv2d(\n",
        "            enhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "        )\n",
        "        d_enhanced_down = tf.nn.conv2d(\n",
        "            enhanced_pool,\n",
        "            self.down_kernel,\n",
        "            strides=[1, 1, 1, 1],\n",
        "            padding=\"SAME\",\n",
        "        )\n",
        "\n",
        "        d_left = tf.square(d_original_left - d_enhanced_left)\n",
        "        d_right = tf.square(d_original_right - d_enhanced_right)\n",
        "        d_up = tf.square(d_original_up - d_enhanced_up)\n",
        "        d_down = tf.square(d_original_down - d_enhanced_down)\n",
        "        return d_left + d_right + d_up + d_down\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQVjf0hd5FFK"
      },
      "source": [
        "### Deep curve estimation model\n",
        "\n",
        "We implement the Zero-DCE framework as a Keras subclassed model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cKShdiWv5FFK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ZeroDCE(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dce_model = build_dce_net()\n",
        "\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "        self.spatial_constancy_loss = SpatialConsistencyLoss(reduction=\"none\")\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.illumination_smoothness_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"illumination_smoothness_loss\"\n",
        "        )\n",
        "        self.spatial_constancy_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"spatial_constancy_loss\"\n",
        "        )\n",
        "        self.color_constancy_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"color_constancy_loss\"\n",
        "        )\n",
        "        self.exposure_loss_tracker = keras.metrics.Mean(name=\"exposure_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.illumination_smoothness_loss_tracker,\n",
        "            self.spatial_constancy_loss_tracker,\n",
        "            self.color_constancy_loss_tracker,\n",
        "            self.exposure_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def get_enhanced_image(self, data, output):\n",
        "        r1 = output[:, :, :, :3]\n",
        "        r2 = output[:, :, :, 3:6]\n",
        "        r3 = output[:, :, :, 6:9]\n",
        "        r4 = output[:, :, :, 9:12]\n",
        "        r5 = output[:, :, :, 12:15]\n",
        "        r6 = output[:, :, :, 15:18]\n",
        "        r7 = output[:, :, :, 18:21]\n",
        "        r8 = output[:, :, :, 21:24]\n",
        "        x = data + r1 * (tf.square(data) - data)\n",
        "        x = x + r2 * (tf.square(x) - x)\n",
        "        x = x + r3 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r4 * (tf.square(x) - x)\n",
        "        x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n",
        "        x = x + r6 * (tf.square(x) - x)\n",
        "        x = x + r7 * (tf.square(x) - x)\n",
        "        enhanced_image = x + r8 * (tf.square(x) - x)\n",
        "        return enhanced_image\n",
        "\n",
        "    def call(self, data):\n",
        "        dce_net_output = self.dce_model(data)\n",
        "        return self.get_enhanced_image(data, dce_net_output)\n",
        "\n",
        "    def compute_losses(self, data, output):\n",
        "        enhanced_image = self.get_enhanced_image(data, output)\n",
        "        loss_illumination = 200 * illumination_smoothness_loss(output)\n",
        "        loss_spatial_constancy = tf.reduce_mean(\n",
        "            self.spatial_constancy_loss(enhanced_image, data)\n",
        "        )\n",
        "        loss_color_constancy = 5 * tf.reduce_mean(color_constancy_loss(enhanced_image))\n",
        "        loss_exposure = 10 * tf.reduce_mean(exposure_loss(enhanced_image))\n",
        "        total_loss = (\n",
        "            loss_illumination\n",
        "            + loss_spatial_constancy\n",
        "            + loss_color_constancy\n",
        "            + loss_exposure\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"total_loss\": total_loss,\n",
        "            \"illumination_smoothness_loss\": loss_illumination,\n",
        "            \"spatial_constancy_loss\": loss_spatial_constancy,\n",
        "            \"color_constancy_loss\": loss_color_constancy,\n",
        "            \"exposure_loss\": loss_exposure,\n",
        "        }\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            output = self.dce_model(data)\n",
        "            losses = self.compute_losses(data, output)\n",
        "\n",
        "        gradients = tape.gradient(\n",
        "            losses[\"total_loss\"], self.dce_model.trainable_weights\n",
        "        )\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
        "\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
        "        self.illumination_smoothness_loss_tracker.update_state(\n",
        "            losses[\"illumination_smoothness_loss\"]\n",
        "        )\n",
        "        self.spatial_constancy_loss_tracker.update_state(\n",
        "            losses[\"spatial_constancy_loss\"]\n",
        "        )\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        output = self.dce_model(data)\n",
        "        losses = self.compute_losses(data, output)\n",
        "\n",
        "        self.total_loss_tracker.update_state(losses[\"total_loss\"])\n",
        "        self.illumination_smoothness_loss_tracker.update_state(\n",
        "            losses[\"illumination_smoothness_loss\"]\n",
        "        )\n",
        "        self.spatial_constancy_loss_tracker.update_state(\n",
        "            losses[\"spatial_constancy_loss\"]\n",
        "        )\n",
        "        self.color_constancy_loss_tracker.update_state(losses[\"color_constancy_loss\"])\n",
        "        self.exposure_loss_tracker.update_state(losses[\"exposure_loss\"])\n",
        "\n",
        "        return {metric.name: metric.result() for metric in self.metrics}\n",
        "\n",
        "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
        "        \"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\n",
        "        self.dce_model.save_weights(\n",
        "            filepath,\n",
        "            overwrite=overwrite,\n",
        "            save_format=save_format,\n",
        "            options=options,\n",
        "        )\n",
        "\n",
        "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
        "        \"\"\"While loading the weights, we simply load the weights of the DCE-Net\"\"\"\n",
        "        self.dce_model.load_weights(\n",
        "            filepath=filepath,\n",
        "            by_name=by_name,\n",
        "            skip_mismatch=skip_mismatch,\n",
        "            options=options,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmfxgNZw5FFL"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1mzjBGhI5FFL",
        "outputId": "3146a6a2-b4a7-4120-cdd6-979854260a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 420ms/step - color_constancy_loss: 0.0042 - exposure_loss: 2.9877 - illumination_smoothness_loss: 1.9564 - spatial_constancy_loss: 7.3591e-06 - total_loss: 4.9483 - val_color_constancy_loss: 0.0024 - val_exposure_loss: 2.9447 - val_illumination_smoothness_loss: 2.3548 - val_spatial_constancy_loss: 1.0665e-05 - val_total_loss: 5.3019\n",
            "Epoch 2/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 210ms/step - color_constancy_loss: 0.0037 - exposure_loss: 2.9793 - illumination_smoothness_loss: 1.1200 - spatial_constancy_loss: 1.5359e-05 - total_loss: 4.1029 - val_color_constancy_loss: 0.0024 - val_exposure_loss: 2.9335 - val_illumination_smoothness_loss: 1.5480 - val_spatial_constancy_loss: 2.4293e-05 - val_total_loss: 4.4838\n",
            "Epoch 3/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 219ms/step - color_constancy_loss: 0.0035 - exposure_loss: 2.9695 - illumination_smoothness_loss: 0.7389 - spatial_constancy_loss: 2.9139e-05 - total_loss: 3.7119 - val_color_constancy_loss: 0.0024 - val_exposure_loss: 2.9219 - val_illumination_smoothness_loss: 1.0985 - val_spatial_constancy_loss: 4.3602e-05 - val_total_loss: 4.0228\n",
            "Epoch 4/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - color_constancy_loss: 0.0035 - exposure_loss: 2.9587 - illumination_smoothness_loss: 0.5234 - spatial_constancy_loss: 4.9619e-05 - total_loss: 3.4857 - val_color_constancy_loss: 0.0025 - val_exposure_loss: 2.9102 - val_illumination_smoothness_loss: 0.8164 - val_spatial_constancy_loss: 6.9035e-05 - val_total_loss: 3.7292\n",
            "Epoch 5/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - color_constancy_loss: 0.0036 - exposure_loss: 2.9476 - illumination_smoothness_loss: 0.3892 - spatial_constancy_loss: 7.6597e-05 - total_loss: 3.3404 - val_color_constancy_loss: 0.0026 - val_exposure_loss: 2.8972 - val_illumination_smoothness_loss: 0.6242 - val_spatial_constancy_loss: 1.0451e-04 - val_total_loss: 3.5241\n",
            "Epoch 6/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - color_constancy_loss: 0.0037 - exposure_loss: 2.9353 - illumination_smoothness_loss: 0.2962 - spatial_constancy_loss: 1.1300e-04 - total_loss: 3.2353 - val_color_constancy_loss: 0.0027 - val_exposure_loss: 2.8838 - val_illumination_smoothness_loss: 0.4806 - val_spatial_constancy_loss: 1.4738e-04 - val_total_loss: 3.3673\n",
            "Epoch 7/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0038 - exposure_loss: 2.9224 - illumination_smoothness_loss: 0.2262 - spatial_constancy_loss: 1.5730e-04 - total_loss: 3.1525 - val_color_constancy_loss: 0.0029 - val_exposure_loss: 2.8684 - val_illumination_smoothness_loss: 0.3711 - val_spatial_constancy_loss: 2.0472e-04 - val_total_loss: 3.2426\n",
            "Epoch 8/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0040 - exposure_loss: 2.9079 - illumination_smoothness_loss: 0.1738 - spatial_constancy_loss: 2.1442e-04 - total_loss: 3.0859 - val_color_constancy_loss: 0.0030 - val_exposure_loss: 2.8514 - val_illumination_smoothness_loss: 0.2910 - val_spatial_constancy_loss: 2.7964e-04 - val_total_loss: 3.1457\n",
            "Epoch 9/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0042 - exposure_loss: 2.8918 - illumination_smoothness_loss: 0.1372 - spatial_constancy_loss: 2.9148e-04 - total_loss: 3.0335 - val_color_constancy_loss: 0.0032 - val_exposure_loss: 2.8322 - val_illumination_smoothness_loss: 0.2367 - val_spatial_constancy_loss: 3.7900e-04 - val_total_loss: 3.0726\n",
            "Epoch 10/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0044 - exposure_loss: 2.8735 - illumination_smoothness_loss: 0.1139 - spatial_constancy_loss: 3.9386e-04 - total_loss: 2.9922 - val_color_constancy_loss: 0.0034 - val_exposure_loss: 2.8096 - val_illumination_smoothness_loss: 0.2008 - val_spatial_constancy_loss: 5.1785e-04 - val_total_loss: 3.0144\n",
            "Epoch 11/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - color_constancy_loss: 0.0046 - exposure_loss: 2.8526 - illumination_smoothness_loss: 0.0991 - spatial_constancy_loss: 5.3207e-04 - total_loss: 2.9568 - val_color_constancy_loss: 0.0037 - val_exposure_loss: 2.7838 - val_illumination_smoothness_loss: 0.1763 - val_spatial_constancy_loss: 7.0627e-04 - val_total_loss: 2.9645\n",
            "Epoch 12/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0050 - exposure_loss: 2.8268 - illumination_smoothness_loss: 0.0905 - spatial_constancy_loss: 7.3414e-04 - total_loss: 2.9231 - val_color_constancy_loss: 0.0040 - val_exposure_loss: 2.7497 - val_illumination_smoothness_loss: 0.1623 - val_spatial_constancy_loss: 0.0010 - val_total_loss: 2.9171\n",
            "Epoch 13/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - color_constancy_loss: 0.0056 - exposure_loss: 2.7924 - illumination_smoothness_loss: 0.0877 - spatial_constancy_loss: 0.0011 - total_loss: 2.8867 - val_color_constancy_loss: 0.0047 - val_exposure_loss: 2.6967 - val_illumination_smoothness_loss: 0.1582 - val_spatial_constancy_loss: 0.0016 - val_total_loss: 2.8612\n",
            "Epoch 14/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0066 - exposure_loss: 2.7338 - illumination_smoothness_loss: 0.0921 - spatial_constancy_loss: 0.0018 - total_loss: 2.8343 - val_color_constancy_loss: 0.0065 - val_exposure_loss: 2.5789 - val_illumination_smoothness_loss: 0.1668 - val_spatial_constancy_loss: 0.0034 - val_total_loss: 2.7555\n",
            "Epoch 15/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 254ms/step - color_constancy_loss: 0.0108 - exposure_loss: 2.5709 - illumination_smoothness_loss: 0.1140 - spatial_constancy_loss: 0.0050 - total_loss: 2.7007 - val_color_constancy_loss: 0.0199 - val_exposure_loss: 2.0764 - val_illumination_smoothness_loss: 0.2383 - val_spatial_constancy_loss: 0.0199 - val_total_loss: 2.3545\n",
            "Epoch 16/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - color_constancy_loss: 0.0453 - exposure_loss: 1.8741 - illumination_smoothness_loss: 0.1963 - spatial_constancy_loss: 0.0467 - total_loss: 2.1625 - val_color_constancy_loss: 0.0822 - val_exposure_loss: 0.7806 - val_illumination_smoothness_loss: 0.2928 - val_spatial_constancy_loss: 0.2163 - val_total_loss: 1.3720\n",
            "Epoch 17/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.1188 - exposure_loss: 0.8583 - illumination_smoothness_loss: 0.2497 - spatial_constancy_loss: 0.2460 - total_loss: 1.4728 - val_color_constancy_loss: 0.0815 - val_exposure_loss: 0.7216 - val_illumination_smoothness_loss: 0.2162 - val_spatial_constancy_loss: 0.2428 - val_total_loss: 1.2621\n",
            "Epoch 18/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0753 - exposure_loss: 0.8907 - illumination_smoothness_loss: 0.1865 - spatial_constancy_loss: 0.2294 - total_loss: 1.3818 - val_color_constancy_loss: 0.0861 - val_exposure_loss: 0.6891 - val_illumination_smoothness_loss: 0.2005 - val_spatial_constancy_loss: 0.2603 - val_total_loss: 1.2360\n",
            "Epoch 19/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 227ms/step - color_constancy_loss: 0.0764 - exposure_loss: 0.8636 - illumination_smoothness_loss: 0.1671 - spatial_constancy_loss: 0.2437 - total_loss: 1.3508 - val_color_constancy_loss: 0.0865 - val_exposure_loss: 0.6892 - val_illumination_smoothness_loss: 0.1815 - val_spatial_constancy_loss: 0.2601 - val_total_loss: 1.2173\n",
            "Epoch 20/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0761 - exposure_loss: 0.8605 - illumination_smoothness_loss: 0.1513 - spatial_constancy_loss: 0.2445 - total_loss: 1.3325 - val_color_constancy_loss: 0.0863 - val_exposure_loss: 0.6894 - val_illumination_smoothness_loss: 0.1718 - val_spatial_constancy_loss: 0.2599 - val_total_loss: 1.2075\n",
            "Epoch 21/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0769 - exposure_loss: 0.8575 - illumination_smoothness_loss: 0.1445 - spatial_constancy_loss: 0.2461 - total_loss: 1.3250 - val_color_constancy_loss: 0.0865 - val_exposure_loss: 0.6889 - val_illumination_smoothness_loss: 0.1627 - val_spatial_constancy_loss: 0.2600 - val_total_loss: 1.1982\n",
            "Epoch 22/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0770 - exposure_loss: 0.8554 - illumination_smoothness_loss: 0.1379 - spatial_constancy_loss: 0.2470 - total_loss: 1.3173 - val_color_constancy_loss: 0.0868 - val_exposure_loss: 0.6889 - val_illumination_smoothness_loss: 0.1553 - val_spatial_constancy_loss: 0.2599 - val_total_loss: 1.1909\n",
            "Epoch 23/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - color_constancy_loss: 0.0770 - exposure_loss: 0.8548 - illumination_smoothness_loss: 0.1331 - spatial_constancy_loss: 0.2475 - total_loss: 1.3124 - val_color_constancy_loss: 0.0869 - val_exposure_loss: 0.6876 - val_illumination_smoothness_loss: 0.1505 - val_spatial_constancy_loss: 0.2606 - val_total_loss: 1.1856\n",
            "Epoch 24/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 228ms/step - color_constancy_loss: 0.0770 - exposure_loss: 0.8544 - illumination_smoothness_loss: 0.1285 - spatial_constancy_loss: 0.2477 - total_loss: 1.3076 - val_color_constancy_loss: 0.0876 - val_exposure_loss: 0.6843 - val_illumination_smoothness_loss: 0.1448 - val_spatial_constancy_loss: 0.2624 - val_total_loss: 1.1791\n",
            "Epoch 25/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0772 - exposure_loss: 0.8522 - illumination_smoothness_loss: 0.1250 - spatial_constancy_loss: 0.2489 - total_loss: 1.3034 - val_color_constancy_loss: 0.0880 - val_exposure_loss: 0.6795 - val_illumination_smoothness_loss: 0.1418 - val_spatial_constancy_loss: 0.2652 - val_total_loss: 1.1746\n",
            "Epoch 26/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0772 - exposure_loss: 0.8521 - illumination_smoothness_loss: 0.1211 - spatial_constancy_loss: 0.2494 - total_loss: 1.2998 - val_color_constancy_loss: 0.0884 - val_exposure_loss: 0.6756 - val_illumination_smoothness_loss: 0.1420 - val_spatial_constancy_loss: 0.2676 - val_total_loss: 1.1735\n",
            "Epoch 27/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0775 - exposure_loss: 0.8504 - illumination_smoothness_loss: 0.1182 - spatial_constancy_loss: 0.2502 - total_loss: 1.2962 - val_color_constancy_loss: 0.0895 - val_exposure_loss: 0.6669 - val_illumination_smoothness_loss: 0.1413 - val_spatial_constancy_loss: 0.2729 - val_total_loss: 1.1707\n",
            "Epoch 28/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 255ms/step - color_constancy_loss: 0.0780 - exposure_loss: 0.8458 - illumination_smoothness_loss: 0.1148 - spatial_constancy_loss: 0.2526 - total_loss: 1.2913 - val_color_constancy_loss: 0.0890 - val_exposure_loss: 0.6685 - val_illumination_smoothness_loss: 0.1398 - val_spatial_constancy_loss: 0.2719 - val_total_loss: 1.1692\n",
            "Epoch 29/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - color_constancy_loss: 0.0778 - exposure_loss: 0.8483 - illumination_smoothness_loss: 0.1106 - spatial_constancy_loss: 0.2512 - total_loss: 1.2879 - val_color_constancy_loss: 0.0901 - val_exposure_loss: 0.6611 - val_illumination_smoothness_loss: 0.1398 - val_spatial_constancy_loss: 0.2766 - val_total_loss: 1.1676\n",
            "Epoch 30/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - color_constancy_loss: 0.0786 - exposure_loss: 0.8415 - illumination_smoothness_loss: 0.1077 - spatial_constancy_loss: 0.2545 - total_loss: 1.2824 - val_color_constancy_loss: 0.0893 - val_exposure_loss: 0.6638 - val_illumination_smoothness_loss: 0.1321 - val_spatial_constancy_loss: 0.2748 - val_total_loss: 1.1600\n",
            "Epoch 31/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - color_constancy_loss: 0.0787 - exposure_loss: 0.8432 - illumination_smoothness_loss: 0.1042 - spatial_constancy_loss: 0.2535 - total_loss: 1.2796 - val_color_constancy_loss: 0.0903 - val_exposure_loss: 0.6599 - val_illumination_smoothness_loss: 0.1317 - val_spatial_constancy_loss: 0.2772 - val_total_loss: 1.1592\n",
            "Epoch 32/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - color_constancy_loss: 0.0789 - exposure_loss: 0.8396 - illumination_smoothness_loss: 0.1013 - spatial_constancy_loss: 0.2552 - total_loss: 1.2751 - val_color_constancy_loss: 0.0898 - val_exposure_loss: 0.6619 - val_illumination_smoothness_loss: 0.1244 - val_spatial_constancy_loss: 0.2758 - val_total_loss: 1.1520\n",
            "Epoch 33/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0790 - exposure_loss: 0.8408 - illumination_smoothness_loss: 0.0980 - spatial_constancy_loss: 0.2546 - total_loss: 1.2724 - val_color_constancy_loss: 0.0908 - val_exposure_loss: 0.6573 - val_illumination_smoothness_loss: 0.1235 - val_spatial_constancy_loss: 0.2788 - val_total_loss: 1.1504\n",
            "Epoch 34/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0791 - exposure_loss: 0.8378 - illumination_smoothness_loss: 0.0955 - spatial_constancy_loss: 0.2563 - total_loss: 1.2687 - val_color_constancy_loss: 0.0905 - val_exposure_loss: 0.6577 - val_illumination_smoothness_loss: 0.1192 - val_spatial_constancy_loss: 0.2784 - val_total_loss: 1.1459\n",
            "Epoch 35/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0793 - exposure_loss: 0.8375 - illumination_smoothness_loss: 0.0922 - spatial_constancy_loss: 0.2563 - total_loss: 1.2653 - val_color_constancy_loss: 0.0909 - val_exposure_loss: 0.6558 - val_illumination_smoothness_loss: 0.1159 - val_spatial_constancy_loss: 0.2797 - val_total_loss: 1.1423\n",
            "Epoch 36/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0797 - exposure_loss: 0.8348 - illumination_smoothness_loss: 0.0893 - spatial_constancy_loss: 0.2579 - total_loss: 1.2617 - val_color_constancy_loss: 0.0899 - val_exposure_loss: 0.6595 - val_illumination_smoothness_loss: 0.1105 - val_spatial_constancy_loss: 0.2771 - val_total_loss: 1.1371\n",
            "Epoch 37/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0796 - exposure_loss: 0.8368 - illumination_smoothness_loss: 0.0858 - spatial_constancy_loss: 0.2565 - total_loss: 1.2588 - val_color_constancy_loss: 0.0912 - val_exposure_loss: 0.6545 - val_illumination_smoothness_loss: 0.1091 - val_spatial_constancy_loss: 0.2804 - val_total_loss: 1.1352\n",
            "Epoch 38/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0798 - exposure_loss: 0.8329 - illumination_smoothness_loss: 0.0838 - spatial_constancy_loss: 0.2587 - total_loss: 1.2552 - val_color_constancy_loss: 0.0914 - val_exposure_loss: 0.6530 - val_illumination_smoothness_loss: 0.1065 - val_spatial_constancy_loss: 0.2814 - val_total_loss: 1.1323\n",
            "Epoch 39/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0799 - exposure_loss: 0.8323 - illumination_smoothness_loss: 0.0809 - spatial_constancy_loss: 0.2590 - total_loss: 1.2521 - val_color_constancy_loss: 0.0914 - val_exposure_loss: 0.6518 - val_illumination_smoothness_loss: 0.1031 - val_spatial_constancy_loss: 0.2822 - val_total_loss: 1.1285\n",
            "Epoch 40/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - color_constancy_loss: 0.0801 - exposure_loss: 0.8310 - illumination_smoothness_loss: 0.0787 - spatial_constancy_loss: 0.2597 - total_loss: 1.2495 - val_color_constancy_loss: 0.0918 - val_exposure_loss: 0.6509 - val_illumination_smoothness_loss: 0.1006 - val_spatial_constancy_loss: 0.2827 - val_total_loss: 1.1260\n",
            "Epoch 41/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0801 - exposure_loss: 0.8304 - illumination_smoothness_loss: 0.0761 - spatial_constancy_loss: 0.2602 - total_loss: 1.2468 - val_color_constancy_loss: 0.0917 - val_exposure_loss: 0.6508 - val_illumination_smoothness_loss: 0.0984 - val_spatial_constancy_loss: 0.2827 - val_total_loss: 1.1236\n",
            "Epoch 42/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0802 - exposure_loss: 0.8303 - illumination_smoothness_loss: 0.0733 - spatial_constancy_loss: 0.2599 - total_loss: 1.2437 - val_color_constancy_loss: 0.0920 - val_exposure_loss: 0.6478 - val_illumination_smoothness_loss: 0.0950 - val_spatial_constancy_loss: 0.2848 - val_total_loss: 1.1196\n",
            "Epoch 43/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0805 - exposure_loss: 0.8277 - illumination_smoothness_loss: 0.0707 - spatial_constancy_loss: 0.2617 - total_loss: 1.2406 - val_color_constancy_loss: 0.0920 - val_exposure_loss: 0.6490 - val_illumination_smoothness_loss: 0.0921 - val_spatial_constancy_loss: 0.2838 - val_total_loss: 1.1169\n",
            "Epoch 44/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 218ms/step - color_constancy_loss: 0.0805 - exposure_loss: 0.8273 - illumination_smoothness_loss: 0.0682 - spatial_constancy_loss: 0.2616 - total_loss: 1.2376 - val_color_constancy_loss: 0.0924 - val_exposure_loss: 0.6473 - val_illumination_smoothness_loss: 0.0887 - val_spatial_constancy_loss: 0.2850 - val_total_loss: 1.1133\n",
            "Epoch 45/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0805 - exposure_loss: 0.8264 - illumination_smoothness_loss: 0.0658 - spatial_constancy_loss: 0.2623 - total_loss: 1.2350 - val_color_constancy_loss: 0.0926 - val_exposure_loss: 0.6462 - val_illumination_smoothness_loss: 0.0868 - val_spatial_constancy_loss: 0.2857 - val_total_loss: 1.1112\n",
            "Epoch 46/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 220ms/step - color_constancy_loss: 0.0807 - exposure_loss: 0.8249 - illumination_smoothness_loss: 0.0643 - spatial_constancy_loss: 0.2630 - total_loss: 1.2329 - val_color_constancy_loss: 0.0926 - val_exposure_loss: 0.6455 - val_illumination_smoothness_loss: 0.0849 - val_spatial_constancy_loss: 0.2861 - val_total_loss: 1.1091\n",
            "Epoch 47/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 218ms/step - color_constancy_loss: 0.0806 - exposure_loss: 0.8251 - illumination_smoothness_loss: 0.0620 - spatial_constancy_loss: 0.2630 - total_loss: 1.2308 - val_color_constancy_loss: 0.0926 - val_exposure_loss: 0.6456 - val_illumination_smoothness_loss: 0.0805 - val_spatial_constancy_loss: 0.2860 - val_total_loss: 1.1047\n",
            "Epoch 48/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 218ms/step - color_constancy_loss: 0.0810 - exposure_loss: 0.8225 - illumination_smoothness_loss: 0.0597 - spatial_constancy_loss: 0.2642 - total_loss: 1.2275 - val_color_constancy_loss: 0.0928 - val_exposure_loss: 0.6455 - val_illumination_smoothness_loss: 0.0788 - val_spatial_constancy_loss: 0.2860 - val_total_loss: 1.1031\n",
            "Epoch 49/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 228ms/step - color_constancy_loss: 0.0808 - exposure_loss: 0.8227 - illumination_smoothness_loss: 0.0579 - spatial_constancy_loss: 0.2642 - total_loss: 1.2258 - val_color_constancy_loss: 0.0929 - val_exposure_loss: 0.6442 - val_illumination_smoothness_loss: 0.0751 - val_spatial_constancy_loss: 0.2869 - val_total_loss: 1.0992\n",
            "Epoch 50/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0812 - exposure_loss: 0.8206 - illumination_smoothness_loss: 0.0557 - spatial_constancy_loss: 0.2653 - total_loss: 1.2228 - val_color_constancy_loss: 0.0929 - val_exposure_loss: 0.6440 - val_illumination_smoothness_loss: 0.0728 - val_spatial_constancy_loss: 0.2870 - val_total_loss: 1.0967\n",
            "Epoch 51/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - color_constancy_loss: 0.0814 - exposure_loss: 0.8192 - illumination_smoothness_loss: 0.0543 - spatial_constancy_loss: 0.2665 - total_loss: 1.2214 - val_color_constancy_loss: 0.0922 - val_exposure_loss: 0.6483 - val_illumination_smoothness_loss: 0.0681 - val_spatial_constancy_loss: 0.2839 - val_total_loss: 1.0925\n",
            "Epoch 52/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0808 - exposure_loss: 0.8235 - illumination_smoothness_loss: 0.0516 - spatial_constancy_loss: 0.2637 - total_loss: 1.2196 - val_color_constancy_loss: 0.0931 - val_exposure_loss: 0.6430 - val_illumination_smoothness_loss: 0.0702 - val_spatial_constancy_loss: 0.2876 - val_total_loss: 1.0939\n",
            "Epoch 53/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 216ms/step - color_constancy_loss: 0.0810 - exposure_loss: 0.8204 - illumination_smoothness_loss: 0.0504 - spatial_constancy_loss: 0.2655 - total_loss: 1.2174 - val_color_constancy_loss: 0.0937 - val_exposure_loss: 0.6412 - val_illumination_smoothness_loss: 0.0673 - val_spatial_constancy_loss: 0.2888 - val_total_loss: 1.0911\n",
            "Epoch 54/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0816 - exposure_loss: 0.8159 - illumination_smoothness_loss: 0.0475 - spatial_constancy_loss: 0.2681 - total_loss: 1.2130 - val_color_constancy_loss: 0.0928 - val_exposure_loss: 0.6455 - val_illumination_smoothness_loss: 0.0613 - val_spatial_constancy_loss: 0.2856 - val_total_loss: 1.0853\n",
            "Epoch 55/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 226ms/step - color_constancy_loss: 0.0814 - exposure_loss: 0.8188 - illumination_smoothness_loss: 0.0452 - spatial_constancy_loss: 0.2662 - total_loss: 1.2115 - val_color_constancy_loss: 0.0934 - val_exposure_loss: 0.6408 - val_illumination_smoothness_loss: 0.0625 - val_spatial_constancy_loss: 0.2890 - val_total_loss: 1.0857\n",
            "Epoch 56/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0815 - exposure_loss: 0.8169 - illumination_smoothness_loss: 0.0446 - spatial_constancy_loss: 0.2674 - total_loss: 1.2104 - val_color_constancy_loss: 0.0937 - val_exposure_loss: 0.6408 - val_illumination_smoothness_loss: 0.0611 - val_spatial_constancy_loss: 0.2889 - val_total_loss: 1.0845\n",
            "Epoch 57/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - color_constancy_loss: 0.0817 - exposure_loss: 0.8150 - illumination_smoothness_loss: 0.0428 - spatial_constancy_loss: 0.2684 - total_loss: 1.2079 - val_color_constancy_loss: 0.0938 - val_exposure_loss: 0.6404 - val_illumination_smoothness_loss: 0.0575 - val_spatial_constancy_loss: 0.2892 - val_total_loss: 1.0808\n",
            "Epoch 58/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - color_constancy_loss: 0.0817 - exposure_loss: 0.8150 - illumination_smoothness_loss: 0.0408 - spatial_constancy_loss: 0.2685 - total_loss: 1.2060 - val_color_constancy_loss: 0.0934 - val_exposure_loss: 0.6422 - val_illumination_smoothness_loss: 0.0554 - val_spatial_constancy_loss: 0.2878 - val_total_loss: 1.0787\n",
            "Epoch 59/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0817 - exposure_loss: 0.8157 - illumination_smoothness_loss: 0.0397 - spatial_constancy_loss: 0.2680 - total_loss: 1.2051 - val_color_constancy_loss: 0.0939 - val_exposure_loss: 0.6388 - val_illumination_smoothness_loss: 0.0572 - val_spatial_constancy_loss: 0.2902 - val_total_loss: 1.0802\n",
            "Epoch 60/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0815 - exposure_loss: 0.8150 - illumination_smoothness_loss: 0.0390 - spatial_constancy_loss: 0.2686 - total_loss: 1.2040 - val_color_constancy_loss: 0.0942 - val_exposure_loss: 0.6392 - val_illumination_smoothness_loss: 0.0548 - val_spatial_constancy_loss: 0.2899 - val_total_loss: 1.0780\n",
            "Epoch 61/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0819 - exposure_loss: 0.8119 - illumination_smoothness_loss: 0.0372 - spatial_constancy_loss: 0.2705 - total_loss: 1.2016 - val_color_constancy_loss: 0.0932 - val_exposure_loss: 0.6443 - val_illumination_smoothness_loss: 0.0484 - val_spatial_constancy_loss: 0.2861 - val_total_loss: 1.0720\n",
            "Epoch 62/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 220ms/step - color_constancy_loss: 0.0816 - exposure_loss: 0.8156 - illumination_smoothness_loss: 0.0354 - spatial_constancy_loss: 0.2679 - total_loss: 1.2005 - val_color_constancy_loss: 0.0941 - val_exposure_loss: 0.6373 - val_illumination_smoothness_loss: 0.0521 - val_spatial_constancy_loss: 0.2912 - val_total_loss: 1.0747\n",
            "Epoch 63/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - color_constancy_loss: 0.0819 - exposure_loss: 0.8118 - illumination_smoothness_loss: 0.0350 - spatial_constancy_loss: 0.2704 - total_loss: 1.1991 - val_color_constancy_loss: 0.0943 - val_exposure_loss: 0.6385 - val_illumination_smoothness_loss: 0.0495 - val_spatial_constancy_loss: 0.2902 - val_total_loss: 1.0725\n",
            "Epoch 64/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - color_constancy_loss: 0.0818 - exposure_loss: 0.8125 - illumination_smoothness_loss: 0.0331 - spatial_constancy_loss: 0.2699 - total_loss: 1.1972 - val_color_constancy_loss: 0.0943 - val_exposure_loss: 0.6373 - val_illumination_smoothness_loss: 0.0481 - val_spatial_constancy_loss: 0.2911 - val_total_loss: 1.0708\n",
            "Epoch 65/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 229ms/step - color_constancy_loss: 0.0818 - exposure_loss: 0.8118 - illumination_smoothness_loss: 0.0324 - spatial_constancy_loss: 0.2702 - total_loss: 1.1963 - val_color_constancy_loss: 0.0948 - val_exposure_loss: 0.6349 - val_illumination_smoothness_loss: 0.0484 - val_spatial_constancy_loss: 0.2928 - val_total_loss: 1.0710\n",
            "Epoch 66/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - color_constancy_loss: 0.0821 - exposure_loss: 0.8103 - illumination_smoothness_loss: 0.0316 - spatial_constancy_loss: 0.2712 - total_loss: 1.1951 - val_color_constancy_loss: 0.0946 - val_exposure_loss: 0.6351 - val_illumination_smoothness_loss: 0.0474 - val_spatial_constancy_loss: 0.2926 - val_total_loss: 1.0698\n",
            "Epoch 67/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - color_constancy_loss: 0.0819 - exposure_loss: 0.8106 - illumination_smoothness_loss: 0.0306 - spatial_constancy_loss: 0.2711 - total_loss: 1.1942 - val_color_constancy_loss: 0.0945 - val_exposure_loss: 0.6381 - val_illumination_smoothness_loss: 0.0422 - val_spatial_constancy_loss: 0.2903 - val_total_loss: 1.0651\n",
            "Epoch 68/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - color_constancy_loss: 0.0826 - exposure_loss: 0.8071 - illumination_smoothness_loss: 0.0298 - spatial_constancy_loss: 0.2733 - total_loss: 1.1928 - val_color_constancy_loss: 0.0925 - val_exposure_loss: 0.6482 - val_illumination_smoothness_loss: 0.0373 - val_spatial_constancy_loss: 0.2830 - val_total_loss: 1.0610\n",
            "Epoch 69/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 219ms/step - color_constancy_loss: 0.0814 - exposure_loss: 0.8165 - illumination_smoothness_loss: 0.0279 - spatial_constancy_loss: 0.2675 - total_loss: 1.1933 - val_color_constancy_loss: 0.0941 - val_exposure_loss: 0.6393 - val_illumination_smoothness_loss: 0.0414 - val_spatial_constancy_loss: 0.2893 - val_total_loss: 1.0641\n",
            "Epoch 70/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - color_constancy_loss: 0.0819 - exposure_loss: 0.8115 - illumination_smoothness_loss: 0.0282 - spatial_constancy_loss: 0.2705 - total_loss: 1.1921 - val_color_constancy_loss: 0.0945 - val_exposure_loss: 0.6368 - val_illumination_smoothness_loss: 0.0431 - val_spatial_constancy_loss: 0.2912 - val_total_loss: 1.0655\n",
            "Epoch 71/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0821 - exposure_loss: 0.8092 - illumination_smoothness_loss: 0.0275 - spatial_constancy_loss: 0.2714 - total_loss: 1.1902 - val_color_constancy_loss: 0.0950 - val_exposure_loss: 0.6332 - val_illumination_smoothness_loss: 0.0419 - val_spatial_constancy_loss: 0.2939 - val_total_loss: 1.0640\n",
            "Epoch 72/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8073 - illumination_smoothness_loss: 0.0261 - spatial_constancy_loss: 0.2726 - total_loss: 1.1883 - val_color_constancy_loss: 0.0950 - val_exposure_loss: 0.6335 - val_illumination_smoothness_loss: 0.0402 - val_spatial_constancy_loss: 0.2936 - val_total_loss: 1.0623\n",
            "Epoch 73/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8073 - illumination_smoothness_loss: 0.0249 - spatial_constancy_loss: 0.2726 - total_loss: 1.1871 - val_color_constancy_loss: 0.0948 - val_exposure_loss: 0.6346 - val_illumination_smoothness_loss: 0.0374 - val_spatial_constancy_loss: 0.2926 - val_total_loss: 1.0595\n",
            "Epoch 74/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8069 - illumination_smoothness_loss: 0.0242 - spatial_constancy_loss: 0.2728 - total_loss: 1.1862 - val_color_constancy_loss: 0.0948 - val_exposure_loss: 0.6356 - val_illumination_smoothness_loss: 0.0393 - val_spatial_constancy_loss: 0.2919 - val_total_loss: 1.0616\n",
            "Epoch 75/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0821 - exposure_loss: 0.8076 - illumination_smoothness_loss: 0.0239 - spatial_constancy_loss: 0.2725 - total_loss: 1.1861 - val_color_constancy_loss: 0.0946 - val_exposure_loss: 0.6351 - val_illumination_smoothness_loss: 0.0363 - val_spatial_constancy_loss: 0.2922 - val_total_loss: 1.0583\n",
            "Epoch 76/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8068 - illumination_smoothness_loss: 0.0230 - spatial_constancy_loss: 0.2726 - total_loss: 1.1848 - val_color_constancy_loss: 0.0951 - val_exposure_loss: 0.6325 - val_illumination_smoothness_loss: 0.0385 - val_spatial_constancy_loss: 0.2941 - val_total_loss: 1.0603\n",
            "Epoch 77/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - color_constancy_loss: 0.0824 - exposure_loss: 0.8057 - illumination_smoothness_loss: 0.0227 - spatial_constancy_loss: 0.2738 - total_loss: 1.1846 - val_color_constancy_loss: 0.0944 - val_exposure_loss: 0.6370 - val_illumination_smoothness_loss: 0.0341 - val_spatial_constancy_loss: 0.2907 - val_total_loss: 1.0562\n",
            "Epoch 78/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0822 - exposure_loss: 0.8076 - illumination_smoothness_loss: 0.0211 - spatial_constancy_loss: 0.2723 - total_loss: 1.1833 - val_color_constancy_loss: 0.0947 - val_exposure_loss: 0.6345 - val_illumination_smoothness_loss: 0.0349 - val_spatial_constancy_loss: 0.2926 - val_total_loss: 1.0566\n",
            "Epoch 79/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0824 - exposure_loss: 0.8062 - illumination_smoothness_loss: 0.0210 - spatial_constancy_loss: 0.2731 - total_loss: 1.1826 - val_color_constancy_loss: 0.0951 - val_exposure_loss: 0.6329 - val_illumination_smoothness_loss: 0.0350 - val_spatial_constancy_loss: 0.2937 - val_total_loss: 1.0567\n",
            "Epoch 80/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0824 - exposure_loss: 0.8049 - illumination_smoothness_loss: 0.0206 - spatial_constancy_loss: 0.2739 - total_loss: 1.1818 - val_color_constancy_loss: 0.0949 - val_exposure_loss: 0.6341 - val_illumination_smoothness_loss: 0.0344 - val_spatial_constancy_loss: 0.2928 - val_total_loss: 1.0562\n",
            "Epoch 81/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8057 - illumination_smoothness_loss: 0.0202 - spatial_constancy_loss: 0.2732 - total_loss: 1.1815 - val_color_constancy_loss: 0.0951 - val_exposure_loss: 0.6319 - val_illumination_smoothness_loss: 0.0340 - val_spatial_constancy_loss: 0.2944 - val_total_loss: 1.0554\n",
            "Epoch 82/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - color_constancy_loss: 0.0827 - exposure_loss: 0.8038 - illumination_smoothness_loss: 0.0198 - spatial_constancy_loss: 0.2745 - total_loss: 1.1808 - val_color_constancy_loss: 0.0953 - val_exposure_loss: 0.6316 - val_illumination_smoothness_loss: 0.0325 - val_spatial_constancy_loss: 0.2946 - val_total_loss: 1.0540\n",
            "Epoch 83/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 253ms/step - color_constancy_loss: 0.0829 - exposure_loss: 0.8020 - illumination_smoothness_loss: 0.0193 - spatial_constancy_loss: 0.2760 - total_loss: 1.1801 - val_color_constancy_loss: 0.0941 - val_exposure_loss: 0.6400 - val_illumination_smoothness_loss: 0.0273 - val_spatial_constancy_loss: 0.2882 - val_total_loss: 1.0496\n",
            "Epoch 84/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 215ms/step - color_constancy_loss: 0.0821 - exposure_loss: 0.8076 - illumination_smoothness_loss: 0.0182 - spatial_constancy_loss: 0.2722 - total_loss: 1.1801 - val_color_constancy_loss: 0.0944 - val_exposure_loss: 0.6363 - val_illumination_smoothness_loss: 0.0304 - val_spatial_constancy_loss: 0.2909 - val_total_loss: 1.0521\n",
            "Epoch 85/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 228ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8065 - illumination_smoothness_loss: 0.0181 - spatial_constancy_loss: 0.2730 - total_loss: 1.1799 - val_color_constancy_loss: 0.0948 - val_exposure_loss: 0.6349 - val_illumination_smoothness_loss: 0.0298 - val_spatial_constancy_loss: 0.2919 - val_total_loss: 1.0515\n",
            "Epoch 86/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8052 - illumination_smoothness_loss: 0.0176 - spatial_constancy_loss: 0.2735 - total_loss: 1.1786 - val_color_constancy_loss: 0.0952 - val_exposure_loss: 0.6313 - val_illumination_smoothness_loss: 0.0302 - val_spatial_constancy_loss: 0.2947 - val_total_loss: 1.0513\n",
            "Epoch 87/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - color_constancy_loss: 0.0829 - exposure_loss: 0.8025 - illumination_smoothness_loss: 0.0174 - spatial_constancy_loss: 0.2752 - total_loss: 1.1780 - val_color_constancy_loss: 0.0951 - val_exposure_loss: 0.6325 - val_illumination_smoothness_loss: 0.0300 - val_spatial_constancy_loss: 0.2937 - val_total_loss: 1.0513\n",
            "Epoch 88/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0824 - exposure_loss: 0.8042 - illumination_smoothness_loss: 0.0173 - spatial_constancy_loss: 0.2740 - total_loss: 1.1780 - val_color_constancy_loss: 0.0950 - val_exposure_loss: 0.6332 - val_illumination_smoothness_loss: 0.0296 - val_spatial_constancy_loss: 0.2931 - val_total_loss: 1.0509\n",
            "Epoch 89/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - color_constancy_loss: 0.0825 - exposure_loss: 0.8040 - illumination_smoothness_loss: 0.0167 - spatial_constancy_loss: 0.2743 - total_loss: 1.1775 - val_color_constancy_loss: 0.0952 - val_exposure_loss: 0.6320 - val_illumination_smoothness_loss: 0.0289 - val_spatial_constancy_loss: 0.2940 - val_total_loss: 1.0501\n",
            "Epoch 90/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0827 - exposure_loss: 0.8029 - illumination_smoothness_loss: 0.0160 - spatial_constancy_loss: 0.2750 - total_loss: 1.1767 - val_color_constancy_loss: 0.0953 - val_exposure_loss: 0.6323 - val_illumination_smoothness_loss: 0.0280 - val_spatial_constancy_loss: 0.2937 - val_total_loss: 1.0494\n",
            "Epoch 91/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 228ms/step - color_constancy_loss: 0.0824 - exposure_loss: 0.8038 - illumination_smoothness_loss: 0.0160 - spatial_constancy_loss: 0.2742 - total_loss: 1.1763 - val_color_constancy_loss: 0.0952 - val_exposure_loss: 0.6312 - val_illumination_smoothness_loss: 0.0294 - val_spatial_constancy_loss: 0.2946 - val_total_loss: 1.0503\n",
            "Epoch 92/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0826 - exposure_loss: 0.8027 - illumination_smoothness_loss: 0.0160 - spatial_constancy_loss: 0.2750 - total_loss: 1.1763 - val_color_constancy_loss: 0.0954 - val_exposure_loss: 0.6316 - val_illumination_smoothness_loss: 0.0277 - val_spatial_constancy_loss: 0.2942 - val_total_loss: 1.0489\n",
            "Epoch 93/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - color_constancy_loss: 0.0826 - exposure_loss: 0.8025 - illumination_smoothness_loss: 0.0157 - spatial_constancy_loss: 0.2750 - total_loss: 1.1757 - val_color_constancy_loss: 0.0956 - val_exposure_loss: 0.6295 - val_illumination_smoothness_loss: 0.0290 - val_spatial_constancy_loss: 0.2957 - val_total_loss: 1.0498\n",
            "Epoch 94/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 214ms/step - color_constancy_loss: 0.0828 - exposure_loss: 0.8016 - illumination_smoothness_loss: 0.0155 - spatial_constancy_loss: 0.2758 - total_loss: 1.1756 - val_color_constancy_loss: 0.0949 - val_exposure_loss: 0.6341 - val_illumination_smoothness_loss: 0.0258 - val_spatial_constancy_loss: 0.2922 - val_total_loss: 1.0470\n",
            "Epoch 95/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 227ms/step - color_constancy_loss: 0.0823 - exposure_loss: 0.8045 - illumination_smoothness_loss: 0.0151 - spatial_constancy_loss: 0.2735 - total_loss: 1.1754 - val_color_constancy_loss: 0.0952 - val_exposure_loss: 0.6307 - val_illumination_smoothness_loss: 0.0285 - val_spatial_constancy_loss: 0.2948 - val_total_loss: 1.0492\n",
            "Epoch 96/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - color_constancy_loss: 0.0827 - exposure_loss: 0.8022 - illumination_smoothness_loss: 0.0152 - spatial_constancy_loss: 0.2751 - total_loss: 1.1752 - val_color_constancy_loss: 0.0957 - val_exposure_loss: 0.6290 - val_illumination_smoothness_loss: 0.0280 - val_spatial_constancy_loss: 0.2961 - val_total_loss: 1.0487\n",
            "Epoch 97/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - color_constancy_loss: 0.0828 - exposure_loss: 0.8012 - illumination_smoothness_loss: 0.0148 - spatial_constancy_loss: 0.2756 - total_loss: 1.1744 - val_color_constancy_loss: 0.0955 - val_exposure_loss: 0.6297 - val_illumination_smoothness_loss: 0.0267 - val_spatial_constancy_loss: 0.2955 - val_total_loss: 1.0474\n",
            "Epoch 98/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - color_constancy_loss: 0.0828 - exposure_loss: 0.8012 - illumination_smoothness_loss: 0.0143 - spatial_constancy_loss: 0.2756 - total_loss: 1.1739 - val_color_constancy_loss: 0.0953 - val_exposure_loss: 0.6304 - val_illumination_smoothness_loss: 0.0271 - val_spatial_constancy_loss: 0.2949 - val_total_loss: 1.0477\n",
            "Epoch 99/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - color_constancy_loss: 0.0827 - exposure_loss: 0.8019 - illumination_smoothness_loss: 0.0140 - spatial_constancy_loss: 0.2755 - total_loss: 1.1741 - val_color_constancy_loss: 0.0953 - val_exposure_loss: 0.6315 - val_illumination_smoothness_loss: 0.0238 - val_spatial_constancy_loss: 0.2940 - val_total_loss: 1.0445\n",
            "Epoch 100/100\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - color_constancy_loss: 0.0825 - exposure_loss: 0.8022 - illumination_smoothness_loss: 0.0135 - spatial_constancy_loss: 0.2748 - total_loss: 1.1731 - val_color_constancy_loss: 0.0955 - val_exposure_loss: 0.6293 - val_illumination_smoothness_loss: 0.0266 - val_spatial_constancy_loss: 0.2957 - val_total_loss: 1.0471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:107: UserWarning: You are saving a model that has not yet been built. It might not contain any weights yet. Consider building the model first by calling it on some data.\n",
            "  return saving_lib.save_model(model, filepath)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Model.save_weights() got an unexpected keyword argument 'save_format'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3091494791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Save only the trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mzero_dce_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zero_dce_weights.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-9-355322911.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m\"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         self.dce_model.save_weights(\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Model.save_weights() got an unexpected keyword argument 'save_format'"
          ]
        }
      ],
      "source": [
        "# zero_dce_model = ZeroDCE()\n",
        "# zero_dce_model.compile(learning_rate=1e-4)\n",
        "# history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n",
        "\n",
        "\n",
        "# def plot_result(item):\n",
        "#     plt.plot(history.history[item], label=item)\n",
        "#     plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "#     plt.xlabel(\"Epochs\")\n",
        "#     plt.ylabel(item)\n",
        "#     plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "#     plt.legend()\n",
        "#     plt.grid()\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# plot_result(\"total_loss\")\n",
        "# plot_result(\"illumination_smoothness_loss\")\n",
        "# plot_result(\"spatial_constancy_loss\")\n",
        "# plot_result(\"color_constancy_loss\")\n",
        "# plot_result(\"exposure_loss\")\n",
        "zero_dce_model = ZeroDCE()\n",
        "zero_dce_model.compile(learning_rate=1e-4)\n",
        "history = zero_dce_model.fit(train_dataset, validation_data=val_dataset, epochs=100)\n",
        "\n",
        "# Save full model (architecture + weights + optimizer state)\n",
        "zero_dce_model.save(\"zero_dce_model.keras\")\n",
        "\n",
        "# Save only the trained weights\n",
        "zero_dce_model.save_weights(\"zero_dce_weights.h5\")\n",
        "\n",
        "\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"total_loss\")\n",
        "plot_result(\"illumination_smoothness_loss\")\n",
        "plot_result(\"spatial_constancy_loss\")\n",
        "plot_result(\"color_constancy_loss\")\n",
        "plot_result(\"exposure_loss\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zero_dce_model.save_weights(\"zero_dce_weights\", save_format='h5')\n"
      ],
      "metadata": {
        "id": "NxoIFncL9l6Y",
        "outputId": "54f3b919-afd9-4c5b-fc81-ea65d09f9415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Model.save_weights() got an unexpected keyword argument 'save_format'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-3341155440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzero_dce_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"zero_dce_weights\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-9-355322911.py\u001b[0m in \u001b[0;36msave_weights\u001b[0;34m(self, filepath, overwrite, save_format, options)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;34m\"\"\"While saving the weights, we simply save the weights of the DCE-Net\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         self.dce_model.save_weights(\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Model.save_weights() got an unexpected keyword argument 'save_format'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p046xscZ5FFL"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wUkwwwLD5FFM"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plot_results(images, titles, figure_size=(12, 12)):\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(len(images)):\n",
        "        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])\n",
        "        _ = plt.imshow(images[i])\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def infer(original_image):\n",
        "    image = keras.utils.img_to_array(original_image)\n",
        "    image = image.astype(\"float32\") / 255.0\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    output_image = zero_dce_model(image)\n",
        "    output_image = tf.cast((output_image[0, :, :, :] * 255), dtype=np.uint8)\n",
        "    output_image = Image.fromarray(output_image.numpy())\n",
        "    return output_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-python tensorflow pillow"
      ],
      "metadata": {
        "id": "Ccfs6cpGDJdp",
        "outputId": "e0a7b1b4-195d-4fc5-ab1d-1249d0761937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPH1RSDM5FFM"
      },
      "source": [
        "### Inference on test images\n",
        "\n",
        "We compare the test images from LOLDataset enhanced by MIRNet with images enhanced via\n",
        "the `PIL.ImageOps.autocontrast()` function.\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/low-light-image-enhancement)\n",
        "and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/low-light-image-enhancement)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fyVMRZpn5FFM",
        "outputId": "35d699f7-8d2a-46b1-8327-e4c1d8f2ce9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4ece524e-ffcf-424c-859c-206d231cd83c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4ece524e-ffcf-424c-859c-206d231cd83c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Low Light Video Enhancement with MIRNet.mp4 to Low Light Video Enhancement with MIRNet.mp4\n",
            "Processing video...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 785/785 [00:27<00:00, 29.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Enhanced video saved to: enhanced_output.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bb430ac7-e081-4209-becf-16743f9e4158\", \"enhanced_output.mp4\", 4018004)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "# from PIL import Image, ImageOps\n",
        "# import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "\n",
        "# # Load the model\n",
        "# # zero_dce_model = tf.keras.models.load_model('path_to_model')\n",
        "\n",
        "# def preprocess_image(image):\n",
        "#     image = image.convert(\"RGB\")\n",
        "#     image = image.resize((640, 480))\n",
        "#     img_array = tf.keras.utils.img_to_array(image) / 255.0\n",
        "#     return tf.expand_dims(img_array, 0)\n",
        "\n",
        "# def infer(image):\n",
        "#     input_tensor = preprocess_image(image)\n",
        "#     output = zero_dce_model(input_tensor, training=False)\n",
        "#     enhanced = tf.clip_by_value(output[0], 0.0, 1.0)\n",
        "#     return tf.keras.utils.array_to_img(enhanced)\n",
        "\n",
        "# def plot_results(images, titles, fig_size=(20, 12)):\n",
        "#     plt.figure(figsize=fig_size)\n",
        "#     for i, (img, title) in enumerate(zip(images, titles)):\n",
        "#         plt.subplot(1, len(images), i + 1)\n",
        "#         plt.imshow(img)\n",
        "#         plt.title(title, fontsize=14)\n",
        "#         plt.axis(\"off\")\n",
        "#     plt.tight_layout()\n",
        "#     plt.show()\n",
        "\n",
        "# # Upload image\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded.keys():\n",
        "#     original_image = Image.open(filename).convert(\"RGB\")\n",
        "#     enhanced_image = infer(original_image)\n",
        "#     plot_results(\n",
        "#         [original_image, ImageOps.autocontrast(original_image), enhanced_image],\n",
        "#         [\"Original\", \"PIL Autocontrast\", \"Enhanced\"]\n",
        "#     )\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Upload a video\n",
        "uploaded = files.upload()\n",
        "input_video_path = list(uploaded.keys())[0]\n",
        "\n",
        "# Load your model\n",
        "# zero_dce_model = tf.keras.models.load_model('path_to_model')  # Load your model here\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    pil_img = Image.fromarray(frame).convert(\"RGB\").resize((640, 480))\n",
        "    img_array = tf.keras.utils.img_to_array(pil_img) / 255.0\n",
        "    return tf.expand_dims(img_array, 0)\n",
        "\n",
        "def enhance_frame(frame):\n",
        "    input_tensor = preprocess_frame(frame)\n",
        "    output = zero_dce_model(input_tensor, training=False)\n",
        "    enhanced = tf.clip_by_value(output[0], 0.0, 1.0).numpy()\n",
        "    enhanced = (enhanced * 255).astype(np.uint8)\n",
        "    return enhanced\n",
        "\n",
        "def process_video(input_video_path, output_video_path):\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(\"Error opening video file\")\n",
        "\n",
        "    width = 640\n",
        "    height = 480\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    print(\"Processing video...\")\n",
        "\n",
        "    for _ in tqdm(range(total_frames)):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frame = cv2.resize(frame, (width, height))\n",
        "        enhanced_frame = enhance_frame(frame)\n",
        "        out.write(cv2.cvtColor(enhanced_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"✅ Enhanced video saved to: {output_video_path}\")\n",
        "\n",
        "# Set output path\n",
        "output_video_path = \"enhanced_output.mp4\"\n",
        "\n",
        "# Process video\n",
        "process_video(input_video_path, output_video_path)\n",
        "\n",
        "# Download the result\n",
        "files.download(output_video_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "zero_dce",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}